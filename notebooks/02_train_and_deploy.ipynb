{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21b0924",
   "metadata": {},
   "source": [
    "# Acoustic Shield - Training & Deployment\n",
    "\n",
    "This notebook trains and deploys the audio classification model using AWS SageMaker.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Setup Configuration** - Define S3 paths, IAM role, and hyperparameters\n",
    "2. **Create Training Job** - Fine-tune wav2vec2-base on audio data\n",
    "3. **Deploy Endpoint** - Deploy real-time inference endpoint\n",
    "4. **Test Endpoint** - Smoke test with sample audio\n",
    "\n",
    "## Dataset Structure\n",
    "Training data must be organized in audiofolder format:\n",
    "```\n",
    "s3://acousticshield-ml/train/\n",
    "‚îú‚îÄ‚îÄ Normal/*.wav\n",
    "‚îú‚îÄ‚îÄ TireSkid/*.wav\n",
    "‚îú‚îÄ‚îÄ EmergencyBraking/*.wav\n",
    "‚îî‚îÄ‚îÄ CollisionImminent/*.wav\n",
    "```\n",
    "\n",
    "## Audio Requirements\n",
    "- **Format**: WAV (mono or stereo)\n",
    "- **Sample Rate**: 16 kHz (auto-resampled if different)\n",
    "- **Duration**: 1-5 seconds recommended\n",
    "\n",
    "## Customization Options\n",
    "- **Change epochs**: Modify `EPOCHS` parameter (default: 4)\n",
    "- **Change learning rate**: Modify `LEARNING_RATE` (default: 3e-5)\n",
    "- **Change batch size**: Modify `BATCH_SIZE` (default: 8)\n",
    "- **Skip validation**: Set `VAL_S3 = None`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e508f5",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: No region hardcoding - SageMaker auto-detects region from bucket location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903894c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85143d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ============================================================================\n",
    "\n",
    "# S3 paths\n",
    "TRAIN_S3 = \"s3://acousticshield-ml/train/\"           # Training data (required)\n",
    "VAL_S3 = \"s3://acousticshield-ml/val/\"               # Validation data (optional, set to None to skip)\n",
    "MODEL_OUTPUT_S3 = \"s3://acousticshield-ml/models/\"   # Model artifacts output\n",
    "\n",
    "# IAM Role\n",
    "ROLE_NAME = \"role-sagemaker-train\"                    # IAM role name (not ARN)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 4                 # Number of training epochs (increase for better accuracy)\n",
    "LEARNING_RATE = 3e-5       # Learning rate (decrease if training is unstable)\n",
    "BATCH_SIZE = 8             # Batch size per device (reduce if OOM errors)\n",
    "WARMUP_STEPS = 500         # Number of warmup steps\n",
    "GRADIENT_ACCUMULATION = 1  # Gradient accumulation steps (increase for effective larger batch)\n",
    "\n",
    "# Instance configuration\n",
    "TRAIN_INSTANCE_TYPE = \"ml.g4dn.xlarge\"  # GPU instance for training\n",
    "TRAIN_INSTANCE_COUNT = 1                 # Number of training instances\n",
    "ENDPOINT_INSTANCE_TYPE = \"ml.m5.xlarge\" # CPU instance for inference\n",
    "ENDPOINT_INSTANCE_COUNT = 1              # Number of endpoint instances\n",
    "\n",
    "# Model configuration\n",
    "TRANSFORMERS_VERSION = \"4.44\"  # HuggingFace Transformers version\n",
    "PYTORCH_VERSION = \"2.3\"        # PyTorch version\n",
    "PYTHON_VERSION = \"py311\"       # Python version\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Acoustic Shield - Training Configuration\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training data: {TRAIN_S3}\")\n",
    "print(f\"Validation data: {VAL_S3 if VAL_S3 else 'None (will split from train)'}\")\n",
    "print(f\"Model output: {MODEL_OUTPUT_S3}\")\n",
    "print(f\"IAM Role: {ROLE_NAME}\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"\\nInstances:\")\n",
    "print(f\"  Training: {TRAIN_INSTANCE_TYPE} x {TRAIN_INSTANCE_COUNT}\")\n",
    "print(f\"  Endpoint: {ENDPOINT_INSTANCE_TYPE} x {ENDPOINT_INSTANCE_COUNT}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55eea8",
   "metadata": {},
   "source": [
    "## Step 2: Initialize SageMaker Session\n",
    "\n",
    "Auto-detect region from S3 bucket location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect region from S3 bucket\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = TRAIN_S3.split('/')[2]  # Extract bucket from s3://bucket/path\n",
    "bucket_location = s3_client.get_bucket_location(Bucket=bucket_name)['LocationConstraint']\n",
    "region = bucket_location if bucket_location else 'us-east-1'\n",
    "\n",
    "print(f\"üåç Detected region: {region}\")\n",
    "\n",
    "# Initialize boto3 session with detected region\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)\n",
    "\n",
    "# Get IAM role ARN\n",
    "iam_client = boto_session.client('iam')\n",
    "role_response = iam_client.get_role(RoleName=ROLE_NAME)\n",
    "TRAIN_ROLE_ARN = role_response['Role']['Arn']\n",
    "\n",
    "print(f\"‚úì SageMaker session initialized\")\n",
    "print(f\"‚úì Region: {region}\")\n",
    "print(f\"‚úì Role ARN: {TRAIN_ROLE_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba3b4e",
   "metadata": {},
   "source": [
    "## Step 3: Create HuggingFace Estimator\n",
    "\n",
    "Configure the training job with the HuggingFace estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='training.py',\n",
    "    role=TRAIN_ROLE_ARN,\n",
    "    instance_type=TRAIN_INSTANCE_TYPE,\n",
    "    instance_count=TRAIN_INSTANCE_COUNT,\n",
    "    transformers_version=TRANSFORMERS_VERSION,\n",
    "    pytorch_version=PYTORCH_VERSION,\n",
    "    py_version=PYTHON_VERSION,\n",
    "    hyperparameters={\n",
    "        'epochs': EPOCHS,\n",
    "        'learning-rate': LEARNING_RATE,\n",
    "        'batch-size': BATCH_SIZE,\n",
    "        'warmup-steps': WARMUP_STEPS,\n",
    "        'gradient-accumulation-steps': GRADIENT_ACCUMULATION,\n",
    "    },\n",
    "    output_path=MODEL_OUTPUT_S3,\n",
    "    base_job_name='acousticshield-train',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    disable_profiler=True,  # Disable profiler to reduce overhead\n",
    "    debugger_hook_config=False,  # Disable debugger to reduce overhead\n",
    ")\n",
    "\n",
    "print(\"‚úì HuggingFace estimator created\")\n",
    "print(f\"  Base job name: acousticshield-train\")\n",
    "print(f\"  Output path: {MODEL_OUTPUT_S3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a64434",
   "metadata": {},
   "source": [
    "## Step 4: Start Training Job\n",
    "\n",
    "‚è±Ô∏è **Expected duration**: 30-40 minutes on ml.g4dn.xlarge\n",
    "\n",
    "The training job will:\n",
    "1. Load audio data from S3 using audiofolder format\n",
    "2. Resample all audio to 16 kHz\n",
    "3. Extract features using wav2vec2 feature extractor\n",
    "4. Fine-tune the model for 4 epochs\n",
    "5. Evaluate on validation set each epoch\n",
    "6. Save best model based on F1 score\n",
    "7. Upload model artifacts to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training channels\n",
    "training_channels = {'train': TRAIN_S3}\n",
    "\n",
    "# Add validation channel if provided\n",
    "if VAL_S3:\n",
    "    training_channels['validation'] = VAL_S3\n",
    "    print(f\"üìä Using separate validation set: {VAL_S3}\")\n",
    "else:\n",
    "    print(f\"üìä Validation set will be split from training data (90/10)\")\n",
    "\n",
    "print(f\"\\nüöÄ Starting training job...\")\n",
    "print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Start training\n",
    "huggingface_estimator.fit(training_channels, wait=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Training job completed!\")\n",
    "print(f\"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üì¶ Model artifacts: {huggingface_estimator.model_data}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa537b",
   "metadata": {},
   "source": [
    "## Step 5: Deploy Real-Time Endpoint\n",
    "\n",
    "‚è±Ô∏è **Expected duration**: 5-8 minutes\n",
    "\n",
    "The endpoint will:\n",
    "- Accept audio/wav input (any sample rate, mono or stereo)\n",
    "- Auto-resample to 16 kHz if needed\n",
    "- Return JSON with label, confidence, and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique endpoint name\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "endpoint_name = f'acousticshield-endpoint-{timestamp}'\n",
    "\n",
    "print(f\"üöÄ Deploying endpoint: {endpoint_name}\")\n",
    "print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üíª Instance: {ENDPOINT_INSTANCE_TYPE}\")\n",
    "print(\"\\nThis will take 5-8 minutes...\\n\")\n",
    "\n",
    "# Deploy endpoint\n",
    "predictor = huggingface_estimator.deploy(\n",
    "    initial_instance_count=ENDPOINT_INSTANCE_COUNT,\n",
    "    instance_type=ENDPOINT_INSTANCE_TYPE,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=DataSerializer(content_type='audio/wav'),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Endpoint deployed successfully!\")\n",
    "print(f\"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üåê Endpoint name: {endpoint_name}\")\n",
    "print(f\"üìç Status: InService\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f15feb",
   "metadata": {},
   "source": [
    "## Step 6: Test Endpoint\n",
    "\n",
    "### Option A: Test with Sample Audio from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample audio from S3 for testing\n",
    "import io\n",
    "\n",
    "# List available test files\n",
    "test_bucket = 'acousticshield-ml'\n",
    "test_prefix = 'train/'  # Or 'val/' if validation set exists\n",
    "\n",
    "s3 = boto3.client('s3', region_name=region)\n",
    "response = s3.list_objects_v2(Bucket=test_bucket, Prefix=test_prefix, MaxKeys=10)\n",
    "\n",
    "if 'Contents' in response:\n",
    "    # Find first WAV file\n",
    "    test_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.wav')]\n",
    "    \n",
    "    if test_files:\n",
    "        test_file_key = test_files[0]\n",
    "        print(f\"üìÅ Using test file: s3://{test_bucket}/{test_file_key}\")\n",
    "        \n",
    "        # Download file\n",
    "        wav_buffer = io.BytesIO()\n",
    "        s3.download_fileobj(test_bucket, test_file_key, wav_buffer)\n",
    "        wav_bytes = wav_buffer.getvalue()\n",
    "        \n",
    "        print(f\"‚úì Downloaded {len(wav_bytes)} bytes\")\n",
    "    else:\n",
    "        print(\"‚ùå No WAV files found in S3 bucket\")\n",
    "        wav_bytes = None\n",
    "else:\n",
    "    print(f\"‚ùå No objects found at s3://{test_bucket}/{test_prefix}\")\n",
    "    wav_bytes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778e08c",
   "metadata": {},
   "source": [
    "### Option B: Generate Synthetic Test Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic test audio (1 second sine wave at 440 Hz)\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "print(\"üéµ Generating synthetic test audio...\")\n",
    "\n",
    "sample_rate = 16000\n",
    "duration = 1.0\n",
    "frequency = 440.0  # A4 note\n",
    "\n",
    "t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "test_audio = 0.3 * np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "# Convert to WAV bytes\n",
    "wav_buffer = io.BytesIO()\n",
    "sf.write(wav_buffer, test_audio, sample_rate, format='WAV')\n",
    "wav_bytes = wav_buffer.getvalue()\n",
    "\n",
    "print(f\"‚úì Generated {len(wav_bytes)} bytes of test audio\")\n",
    "print(f\"  Format: 16 kHz mono, {duration} second sine wave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81243e",
   "metadata": {},
   "source": [
    "### Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wav_bytes:\n",
    "    print(\"\\nüîÆ Invoking endpoint...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Predict using the deployed endpoint\n",
    "    response = predictor.predict(wav_bytes)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Prediction Results:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üè∑Ô∏è  PREDICTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Predicted Class: {response['label']}\")\n",
    "    print(f\"Confidence: {response['confidence']:.2%}\")\n",
    "    print(\"\\nClass Probabilities:\")\n",
    "    for class_name, prob in sorted(response['probs'].items(), key=lambda x: x[1], reverse=True):\n",
    "        bar = '‚ñà' * int(prob * 50)\n",
    "        print(f\"  {class_name:20s} {prob:.2%} {bar}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n‚úÖ Endpoint test successful!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No test audio available. Please provide a WAV file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decb74e",
   "metadata": {},
   "source": [
    "## Step 7: Test with boto3 SageMaker Runtime (Alternative Method)\n",
    "\n",
    "This demonstrates how to invoke the endpoint using raw boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca69199",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wav_bytes:\n",
    "    print(\"üîß Testing with boto3 SageMaker Runtime client...\\n\")\n",
    "    \n",
    "    # Create SageMaker Runtime client\n",
    "    runtime_client = boto_session.client('sagemaker-runtime')\n",
    "    \n",
    "    # Invoke endpoint\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='audio/wav',\n",
    "        Accept='application/json',\n",
    "        Body=wav_bytes\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    \n",
    "    print(\"üìä Response from boto3 client:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    print(f\"\\n‚úÖ boto3 invocation successful!\")\n",
    "    print(f\"   Predicted: {result['label']} ({result['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323892e1",
   "metadata": {},
   "source": [
    "## Step 8: Endpoint Information\n",
    "\n",
    "Save endpoint details for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ENDPOINT INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Endpoint Name: {endpoint_name}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Instance Type: {ENDPOINT_INSTANCE_TYPE}\")\n",
    "print(f\"Instance Count: {ENDPOINT_INSTANCE_COUNT}\")\n",
    "print(f\"Model Artifacts: {huggingface_estimator.model_data}\")\n",
    "print(f\"\\nInput Format: audio/wav (16 kHz mono recommended, auto-resampled)\")\n",
    "print(f\"Output Format: application/json\")\n",
    "print(f\"\\nExpected Output:\")\n",
    "print(f\"  {{\")\n",
    "print(f\"    \\\"label\\\": \\\"TireSkid\\\",\")\n",
    "print(f\"    \\\"confidence\\\": 0.85,\")\n",
    "print(f\"    \\\"probs\\\": {{\")\n",
    "print(f\"      \\\"Normal\\\": 0.05,\")\n",
    "print(f\"      \\\"TireSkid\\\": 0.85,\")\n",
    "print(f\"      \\\"EmergencyBraking\\\": 0.08,\")\n",
    "print(f\"      \\\"CollisionImminent\\\": 0.02\")\n",
    "print(f\"    }}\")\n",
    "print(f\"  }}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save endpoint info to file\n",
    "endpoint_info = {\n",
    "    'endpoint_name': endpoint_name,\n",
    "    'region': region,\n",
    "    'instance_type': ENDPOINT_INSTANCE_TYPE,\n",
    "    'model_artifacts': huggingface_estimator.model_data,\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'classes': ['Normal', 'TireSkid', 'EmergencyBraking', 'CollisionImminent']\n",
    "}\n",
    "\n",
    "with open('endpoint_info.json', 'w') as f:\n",
    "    json.dump(endpoint_info, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Endpoint information saved to endpoint_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353f918",
   "metadata": {},
   "source": [
    "## Optional: Cleanup\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: This will delete the endpoint. You will be charged while the endpoint is running.\n",
    "\n",
    "Uncomment and run the cell below to delete the endpoint when done testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87380ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to delete endpoint\n",
    "# print(f\"üóëÔ∏è  Deleting endpoint: {endpoint_name}\")\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"‚úì Endpoint deleted successfully\")\n",
    "# print(\"\\n‚ö†Ô∏è  Model artifacts remain in S3 and can be redeployed anytime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eebd7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "‚úÖ **Training Complete**: Model fine-tuned on audio classification task  \n",
    "‚úÖ **Endpoint Deployed**: Real-time inference endpoint is running  \n",
    "‚úÖ **Testing Complete**: Endpoint responds with predictions  \n",
    "\n",
    "### Next Steps\n",
    "1. Integrate endpoint into your application\n",
    "2. Monitor endpoint metrics in CloudWatch\n",
    "3. Set up auto-scaling if needed\n",
    "4. Retrain periodically with new data\n",
    "\n",
    "### Cost Management\n",
    "- **Training**: One-time cost (~$0.50-1.00)\n",
    "- **Endpoint**: Ongoing cost (~$0.23/hour for ml.m5.xlarge)\n",
    "- **Storage**: Model artifacts in S3 (~$0.02/month)\n",
    "\n",
    "üí° **Tip**: Delete the endpoint when not in use and redeploy when needed to save costs!\n",
    "\n",
    "### Documentation\n",
    "- Endpoint name saved in `endpoint_info.json`\n",
    "- Model artifacts: `s3://acousticshield-ml/models/`\n",
    "- Training logs: Available in CloudWatch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
