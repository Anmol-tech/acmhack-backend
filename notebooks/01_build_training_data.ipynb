{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016ca6aa",
   "metadata": {},
   "source": [
    "# Acoustic Shield - Training Data Pipeline\n",
    "\n",
    "This notebook orchestrates the complete data pipeline:\n",
    "1. Extract crash hotspots from GeoJSON\n",
    "2. Enrich with weather data (Open-Meteo API)\n",
    "3. Synthesize risk events\n",
    "4. Build audio generation recipes\n",
    "5. Run SageMaker Processing job to generate WAV files\n",
    "6. Validate outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1867be2",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import boto3\n",
    "from sagemaker import Session\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "from data_pipeline import (\n",
    "    S3Client,\n",
    "    HotspotExtractor,\n",
    "    WeatherEnricher,\n",
    "    RiskEventSynthesizer,\n",
    "    RecipeBuilder\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - No hard-coded regions!\n",
    "RAW_BUCKET = 'acousticshield-raw'\n",
    "ML_BUCKET = 'acousticshield-ml'\n",
    "CRASH_FILE_KEY = 'crash_hotspots/sanjose_crashes.geojson'\n",
    "SAGEMAKER_ROLE = 'role-sagemaker-processing'\n",
    "\n",
    "# Processing parameters\n",
    "TOP_N_HOTSPOTS = 25\n",
    "EVENTS_PER_HOTSPOT = 4\n",
    "\n",
    "# Get region from bucket\n",
    "s3_client = S3Client()\n",
    "REGION = s3_client.get_bucket_region(RAW_BUCKET)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Raw bucket: {RAW_BUCKET}\")\n",
    "print(f\"  ML bucket: {ML_BUCKET}\")\n",
    "print(f\"  Region: {REGION}\")\n",
    "print(f\"  Crash file: s3://{RAW_BUCKET}/{CRASH_FILE_KEY}\")\n",
    "print(f\"  Top hotspots: {TOP_N_HOTSPOTS}\")\n",
    "print(f\"  Events per hotspot: {EVENTS_PER_HOTSPOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdde042",
   "metadata": {},
   "source": [
    "## Step 1: Extract Crash Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load crash data from S3\n",
    "logger.info(f\"Loading crash data from s3://{RAW_BUCKET}/{CRASH_FILE_KEY}\")\n",
    "crash_data = s3_client.read_json(RAW_BUCKET, CRASH_FILE_KEY)\n",
    "\n",
    "# Extract hotspots\n",
    "extractor = HotspotExtractor(crash_data)\n",
    "hotspots = extractor.extract_top_hotspots(top_n=TOP_N_HOTSPOTS)\n",
    "\n",
    "# Get summary stats\n",
    "stats = extractor.get_summary_stats()\n",
    "print(f\"\\nüìä Crash Data Summary:\")\n",
    "print(f\"  Total crashes: {stats['total_crashes']}\")\n",
    "print(f\"  Total injuries: {stats['total_injuries']}\")\n",
    "print(f\"  Total fatalities: {stats['total_fatalities']}\")\n",
    "print(f\"  Speeding involved: {stats['speeding_involved_pct']:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Top 5 Hotspots:\")\n",
    "for hotspot in hotspots[:5]:\n",
    "    print(f\"  {hotspot['rank']}. {hotspot['location_name']}: {hotspot['crash_count']} crashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855c995",
   "metadata": {},
   "source": [
    "## Step 2: Enrich with Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich hotspots with weather data from Open-Meteo API\n",
    "logger.info(\"Fetching weather data for hotspots...\")\n",
    "enricher = WeatherEnricher()\n",
    "enriched_hotspots = enricher.enrich_hotspots(hotspots, rate_limit_delay=0.5)\n",
    "\n",
    "# Show sample enriched data\n",
    "print(f\"\\nüå§Ô∏è  Sample Enriched Hotspot:\")\n",
    "sample = enriched_hotspots[0]\n",
    "print(f\"  Location: {sample['location_name']}\")\n",
    "print(f\"  Crashes: {sample['crash_count']}\")\n",
    "print(f\"  Weather:\")\n",
    "weather = sample['weather']\n",
    "print(f\"    Temperature: {weather['temperature_c']:.1f}¬∞C\")\n",
    "print(f\"    Rain: {weather['rain_mm']:.1f}mm\")\n",
    "print(f\"    Wind: {weather['wind_speed_kmh']:.1f} km/h\")\n",
    "print(f\"    Risk: {enricher.categorize_weather_risk(weather)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d68f76",
   "metadata": {},
   "source": [
    "## Step 3: Synthesize Risk Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic risk events\n",
    "logger.info(\"Synthesizing risk events...\")\n",
    "synthesizer = RiskEventSynthesizer(seed=42)\n",
    "risk_events = synthesizer.synthesize_events(enriched_hotspots, events_per_hotspot=EVENTS_PER_HOTSPOT)\n",
    "\n",
    "# Get distribution\n",
    "distribution = synthesizer.get_event_distribution(risk_events)\n",
    "print(f\"\\n‚ö†Ô∏è  Risk Event Distribution:\")\n",
    "print(f\"  Total events: {distribution['total_events']}\")\n",
    "print(f\"  By risk type:\")\n",
    "for risk_type, count in distribution['risk_type_distribution'].items():\n",
    "    print(f\"    {risk_type}: {count}\")\n",
    "print(f\"  By weather risk:\")\n",
    "for weather_risk, count in distribution['weather_risk_distribution'].items():\n",
    "    print(f\"    {weather_risk}: {count}\")\n",
    "\n",
    "# Show sample event\n",
    "print(f\"\\nüìã Sample Risk Event:\")\n",
    "sample_event = risk_events[0]\n",
    "print(json.dumps(sample_event, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63018e86",
   "metadata": {},
   "source": [
    "## Step 4: Build Audio Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d423e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build audio generation recipes\n",
    "logger.info(\"Building audio recipes...\")\n",
    "builder = RecipeBuilder()\n",
    "recipes = builder.build_recipes(risk_events)\n",
    "\n",
    "# Get summary\n",
    "summary = builder.get_recipe_summary(recipes)\n",
    "print(f\"\\nüéµ Audio Recipe Summary:\")\n",
    "print(f\"  Total recipes: {summary['total_recipes']}\")\n",
    "print(f\"  Total audio duration: {summary['total_audio_duration_minutes']:.2f} minutes\")\n",
    "print(f\"  By risk type:\")\n",
    "for risk_type, count in summary['risk_type_distribution'].items():\n",
    "    print(f\"    {risk_type}: {count} recipes\")\n",
    "\n",
    "# Show sample recipe\n",
    "print(f\"\\nüéº Sample Recipe:\")\n",
    "sample_recipe = recipes[0]\n",
    "print(json.dumps(sample_recipe, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ab018",
   "metadata": {},
   "source": [
    "## Step 5: Save Intermediate Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52765b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save risk events to S3\n",
    "logger.info(\"Saving risk events to S3...\")\n",
    "risk_events_key = 'risk_events/risk_events.json'\n",
    "s3_path = s3_client.write_json(risk_events, RAW_BUCKET, risk_events_key)\n",
    "print(f\"‚úì Risk events saved to: {s3_path}\")\n",
    "\n",
    "# Save recipes to S3\n",
    "logger.info(\"Saving recipes to S3...\")\n",
    "recipes_key = 'prompts/audio_recipes.json'\n",
    "s3_path = s3_client.write_json(recipes, RAW_BUCKET, recipes_key)\n",
    "print(f\"‚úì Recipes saved to: {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a681ea6",
   "metadata": {},
   "source": [
    "## Step 6: Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate AI-Enhanced Audio WAV Files\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéµ STEP 5: GENERATE AI-ENHANCED AUDIO WAV FILES\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using Bedrock AI to optimize audio parameters for realistic sound\")\n",
    "\n",
    "try:\n",
    "    from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "    processor = ScriptProcessor(\n",
    "        role=role,\n",
    "        image_uri=f'763104351884.dkr.ecr.{REGION}.amazonaws.com/pytorch-training:2.0.0-cpu-py310',\n",
    "        command=['python3'],\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        base_job_name='acousticshield-audio-gen',\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    # Generate WAV files with Bedrock AI enhancement (default enabled)\n",
    "    processor.run(\n",
    "        inputs=[ProcessingInput(source=f's3://{ML_BUCKET}/recipes/train/', destination='/opt/ml/processing/input')],\n",
    "        outputs=[ProcessingOutput(output_name='audio_wav', source='/opt/ml/processing/output', destination=f's3://{ML_BUCKET}/train/')],\n",
    "        code='../processing/bedrock_audio_generator.py',\n",
    "        arguments=['--region', REGION]  # AI enabled by default, use --no-ai to disable\n",
    "    )\n",
    "    print(\"‚úÖ AI-enhanced WAV file generation complete!\")\n",
    "    print(\"üí° Audio parameters optimized by Bedrock Claude AI\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processing job with Bedrock\n",
    "logger.info(\"Starting SageMaker processing job with Bedrock...\")\n",
    "\n",
    "processor.run(\n",
    "    code='../processing/bedrock_audio_generator.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f's3://{RAW_BUCKET}/{recipes_key}',\n",
    "            destination='/opt/ml/processing/input'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination=f's3://{ML_BUCKET}/train/',\n",
    "            output_name='audio_metadata'\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        '--recipe-dir', '/opt/ml/processing/input',\n",
    "        '--output-dir', '/opt/ml/processing/output',\n",
    "        '--region', REGION\n",
    "    ],\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Bedrock audio metadata generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1de0b",
   "metadata": {},
   "source": [
    "## Step 7: Validate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c64575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated WAV files\n",
    "s3_client = boto3.client('s3', region_name=REGION)\n",
    "wav_files = []\n",
    "\n",
    "response = s3_client.list_objects_v2(Bucket=ML_BUCKET, Prefix='train/')\n",
    "if 'Contents' in response:\n",
    "    wav_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.wav')]\n",
    "\n",
    "print(f\"\\nüéµ Generated WAV Files: {len(wav_files)}\")\n",
    "if wav_files:\n",
    "    print(f\"First 10 files:\")\n",
    "    for f in wav_files[:10]:\n",
    "        print(f\"  {f}\")\n",
    "\n",
    "if len(wav_files) > 10:\n",
    "    print(f\"    ... and {len(wav_files) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d50d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count WAV files by risk type\n",
    "risk_type_counts = {'normal': 0, 'tireskid': 0, 'emergencybraking': 0, 'collisionimminent': 0}\n",
    "\n",
    "for wav_file in wav_files:\n",
    "    filename = wav_file.lower()\n",
    "    for risk_type in risk_type_counts.keys():\n",
    "        if risk_type in filename:\n",
    "            risk_type_counts[risk_type] += 1\n",
    "            break\n",
    "\n",
    "print(f\"\\nüìä WAV Files by Risk Type:\")\n",
    "for risk_type, count in risk_type_counts.items():\n",
    "    print(f\"  {risk_type.title()}: {count} files\")\n",
    "\n",
    "# Show sample file info\n",
    "if wav_files:\n",
    "    print(f\"\\nüìã Sample WAV File:\")\n",
    "    sample_key = wav_files[0]\n",
    "    obj = s3_client.head_object(Bucket=ML_BUCKET, Key=sample_key)\n",
    "    print(f\"  File: {sample_key}\")\n",
    "    print(f\"  Size: {obj['ContentLength']:,} bytes\")\n",
    "    print(f\"  Duration: ~5 seconds @ 22.05kHz\")\n",
    "    print(f\"  Format: 16-bit PCM WAV\")\n",
    "    print(f\"\\n‚úÖ Files are ready for ML training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9759dd9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ACOUSTIC SHIELD DATA PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìç Crash Hotspots Analyzed: {len(hotspots)}\")\n",
    "print(f\"‚ö†Ô∏è  Risk Events Generated: {len(risk_events)}\")\n",
    "print(f\"üéµ Audio Recipes Created: {len(recipes)}\")\n",
    "print(f\"ü§ñ AI-Enhanced WAV Files: {len(wav_files)}\")\n",
    "print(f\"\\nüíæ Data Locations:\")\n",
    "print(f\"  Risk Events: s3://{RAW_BUCKET}/{risk_events_key}\")\n",
    "print(f\"  Recipes: s3://{RAW_BUCKET}/{recipes_key}\")\n",
    "print(f\"  Audio Files: s3://{ML_BUCKET}/train/\")\n",
    "print(f\"\\n‚úÖ Ready for ML model training!\")\n",
    "print(f\"üß† All audio parameters optimized by Bedrock AI\")\n",
    "print(f\"üéØ Each risk type has distinct audio characteristics\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
